å¤šæ¨¡æ€:
https://zhuanlan.zhihu.com/p/699376156
https://www.zhihu.com/search?type=content&q=clip
https://zhuanlan.zhihu.com/p/493489688
https://github.com/openai/CLIP
https://zhuanlan.zhihu.com/p/593661212
https://www.bilibili.com/video/BV1xM4m1m7vA/?spm_id_from=333.999.0.0&vd_source=c9745e4447536b28b2b0735071d30bd6
https://github.com/yunhao-tech/Course_project/blob/master/Advanced%20Machine%20learning/Final%20project_CLIP.ipynb
https://zhuanlan.zhihu.com/p/588853559
https://zhuanlan.zhihu.com/p/653902791
https://zhuanlan.zhihu.com/p/651116964
https://zhuanlan.zhihu.com/p/676480190
https://zhuanlan.zhihu.com/p/660476765

cuda-mode:
https://github.com/cuda-mode/lectures/tree/main/lecture_002
https://zhuanlan.zhihu.com/p/708682239
https://zhuanlan.zhihu.com/p/707107808
https://github.com/cuda-mode

flashattention:
https://zhuanlan.zhihu.com/p/664061672
https://zhuanlan.zhihu.com/p/668888063
https://tridao.me/blog/2024/flash3/
https://github.com/facebookresearch/xformers
https://zhuanlan.zhihu.com/p/642962397
https://zhuanlan.zhihu.com/p/669926191

sequence parallel:
https://zhuanlan.zhihu.com/p/703669087
https://zhuanlan.zhihu.com/p/700639611

k8s:
https://open.atatech.org/articles/11000218484?spm=ata.28742492.0.0.45543459cz1WZN
https://mp.weixin.qq.com/s/4l0g2VI_18EuOjaYQuHYHw?spm=ata.28742492.0.0.68e33fedZgB6Op
https://open.atatech.org/articles/11000266664?spm=ata.28742492.0.0.45543459cz1WZN

rdma:
https://open.atatech.org/articles/11020264428?spm=ata.28742492.0.0.45543459cz1WZN

cuda learning & inference:
DefTruth/CUDA-Learn-Notes: ğŸ‰CUDA ç¬”è®° / å¤§æ¨¡å‹æ‰‹æ’•CUDA / C++ç¬”è®°ï¼Œæ›´æ–°éšç¼˜: flash_attnã€sgemmã€sgemvã€warp reduceã€block reduceã€dot productã€elementwiseã€softmaxã€layernormã€rmsnormã€hist etc.
https://github.com/DefTruth/CUDA-Learn-Notes

[TensorRT-LLM][5wå­—]ğŸ”¥TensorRT-LLM éƒ¨ç½²è°ƒä¼˜-æŒ‡åŒ— - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/699333691

[Attentionä¼˜åŒ–][ä¸‡å­—]ğŸ”¥TensorRT 9.2 MHA/Myelin Optimize vs FlashAttention-2 profile - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/678873216

[Attentionä¼˜åŒ–][2wå­—]ğŸ”¥åŸç†&å›¾è§£: ä»Online-Softmaxåˆ°FlashAttention V1/V2/V3 - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/668888063

CUDAç¼–ç¨‹å…¥é—¨ä¹‹Warp-Level Primitives - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/572820783

CUDAç¼–ç¨‹å…¥é—¨ä¹‹Vectorized Memory Access - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/572817996

CUDA C++ Best Practices Guide
https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html?highlight=bank%20conflict#shared-memory-and-memory-banks

CUB â€” cub 2.5 documentation
https://nvidia.github.io/cccl/cub/

NVIDIA/FasterTransformer: Transformer related optimization, including BERT, GPT
https://github.com/NVIDIA/FasterTransformer/tree/main

llm.cä»£ç ç®€è¯»ï¼ˆä¸€ï¼‰ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/701379487

karpathy/llm.c: LLM training in simple, raw C/CUDA
https://github.com/karpathy/llm.c

å¾®ä¿¡å…¬ä¼—å¹³å°
https://mp.weixin.qq.com/s/-kVLJnBKEDV-TbCZH1UBTw

Tensor Cores ä½¿ç”¨ä»‹ç» - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/671312675

LLMï¼ˆåä¸ƒï¼‰ï¼šä» FlashAttention åˆ° PagedAttention, å¦‚ä½•è¿›ä¸€æ­¥ä¼˜åŒ– Attention æ€§èƒ½ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/638468472

ring attention + flash attentionï¼šè¶…é•¿ä¸Šä¸‹æ–‡ä¹‹è·¯ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/683714620

å›½å†…å¤§å‚GPU CUDAé«˜é¢‘é¢è¯•é—®é¢˜æ±‡æ€»ï¼ˆå«éƒ¨åˆ†ç­”æ¡ˆï¼‰ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/678602674

CUDAå¼€å‘æ€»ç»“ç¬”è®° - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/570795544

é«˜æ€§èƒ½è®¡ç®—æ–¹å‘é¢è¯•é—®é¢˜æ€»ç»“ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/634557901

[LLMæ¨ç†ä¼˜åŒ–][3wå­—]ğŸ”¥é«˜é¢‘é¢è¯•é¢˜æ±‡æ€»-å¤§æ¨¡å‹æ‰‹æ’•CUDA - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/678903537?utm_campaign=

ä¸€æ–‡è¯»æ‡‚nsight systemä¸cuda kernelçš„æ—¶é—´çº¿åˆ†æä¸å¯è§†åŒ– - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/691307737

Using CUDA Warp-Level Primitives | NVIDIA Technical Blog
https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/

Pytorch CUDAæºç è§£æ - BlockReduceSum - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/584936904

pytorch/aten/src/ATen/native/cuda/block_reduce.cuh at main Â· pytorch/pytorch
https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cuda/block_reduce.cuh#L71

æ·±å…¥ç†è§£Pytorchæºç ä¸­çš„CUDAç®—å­01-BlockReduceSum | SunGeng Blog
https://sungenglab.github.io/post/2024-04-27pytorch%E7%AE%97%E5%AD%90%E5%88%86%E6%9E%90/

Programming Tensor Cores in CUDA 9 | NVIDIA Technical Blog
https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/


Megatronå­¦ä¹ :

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (2) --- æ•´ä½“æ¶æ„ - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15868988.html

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (3) ---æ¨¡å‹å¹¶è¡Œå®ç° - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15871062.html

Megatron-LMæºç é˜…è¯»ï¼ˆä¸€ï¼‰ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/405883984

Megatron-LMæºç é˜…è¯»ï¼ˆäºŒï¼‰ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/407094090

[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(1) - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/366906920

NVIDIA/Megatron-LM: Ongoing research training transformer models at scale
https://github.com/NVIDIA/Megatron-LM

megatronæ¡†æ¶-å“”å“©å“”å“©_bilibili
https://search.bilibili.com/all?keyword=megatron%E6%A1%86%E6%9E%B6&from_source=webtop_search&spm_id_from=333.1007&search_source=5

chenzomi12/AISystem: AISystem ä¸»è¦æ˜¯æŒ‡AIç³»ç»Ÿï¼ŒåŒ…æ‹¬AIèŠ¯ç‰‡ã€AIç¼–è¯‘å™¨ã€AIæ¨ç†å’Œè®­ç»ƒæ¡†æ¶ç­‰AIå…¨æ ˆåº•å±‚æŠ€æœ¯
https://github.com/chenzomi12/AISystem/tree/main

ZOMIé…±çš„ä¸ªäººç©ºé—´-ZOMIé…±ä¸ªäººä¸»é¡µ-å“”å“©å“”å“©è§†é¢‘
https://space.bilibili.com/517221395

åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶Megatron-LMä»£ç æ¦‚è§ˆ #å¤§æ¨¡å‹ #åˆ†å¸ƒå¼å¹¶è¡Œ #è®­ç»ƒ_å“”å“©å“”å“©_bilibili
https://www.bilibili.com/video/BV12J4m1K78y/?spm_id_from=333.788&vd_source=559e617af2d73e0db5172ab4e445c28e

å›¾è§£å¤§æ¨¡å‹è®­ç»ƒä¹‹ï¼šå¼ é‡æ¨¡å‹å¹¶è¡Œ(TP)ï¼ŒMegatron-LM - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/622212228

å›¾è§£å¤§æ¨¡å‹è®­ç»ƒç³»åˆ—ä¹‹ï¼šMegatronæºç è§£è¯»3ï¼Œåˆ†å¸ƒå¼æ··åˆç²¾åº¦è®­ç»ƒ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/662700424

å›¾è§£å¤§æ¨¡å‹ç³»åˆ—ä¹‹ï¼šMegatronæºç è§£è¯»1ï¼Œåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ– - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/629121480

å›¾è§£å¤§æ¨¡å‹è®­ç»ƒä¹‹ï¼šå¼ é‡æ¨¡å‹å¹¶è¡Œ(TP)ï¼ŒMegatron-LM - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/622212228

å›¾è§£å¤§æ¨¡å‹ç³»åˆ—ä¹‹ï¼šMegatronæºç è§£è¯»1ï¼Œåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ– - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/629121480

å›¾è§£å¤§æ¨¡å‹è®­ç»ƒç³»åˆ—ä¹‹ï¼šDeepSpeed-Megatron MoEå¹¶è¡Œè®­ç»ƒï¼ˆåŸç†ç¯‡ï¼‰ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/681154742

å›¾è§£å¤§æ¨¡å‹è®­ç»ƒä¹‹ï¼šMegatronæºç è§£è¯»2ï¼Œæ¨¡å‹å¹¶è¡Œ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/634377071

éœ‡æƒŠï¼æˆ‘ç«Ÿç„¶åœ¨1080Tiä¸ŠåŠ è½½äº†ä¸€ä¸ª35äº¿å‚æ•°çš„æ¨¡å‹ï¼ˆZeRO, Zero Redundancy Optimizerï¼‰_wx5e46005fc4d21çš„æŠ€æœ¯åšå®¢_51CTOåšå®¢
https://blog.51cto.com/u_14691718/5631471

å¦‚ä½•è§£å†³æ··åˆç²¾åº¦è®­ç»ƒå¤§æ¨¡å‹çš„å±€é™æ€§é—®é¢˜-æ··åˆç²¾åº¦è®­ç»ƒ tensorflow
https://www.51cto.com/article/746136.html

Distributed communication package - torch.distributed â€” PyTorch 2.3 documentation
https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group

[æºç è§£æ] PyTorchåˆ†å¸ƒå¼(6) -------- DistributedDataParallel -- init_method & store - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossixyz/p/15553670.html

[æºç è§£æ] PyTorch æµæ°´çº¿å¹¶è¡Œå®ç° (6)--å¹¶è¡Œè®¡ç®— - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15370731.html

ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ?page=2

[æºç è§£æ] PyTorch æµæ°´çº¿å¹¶è¡Œå®ç° (1)--åŸºç¡€çŸ¥è¯† - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15318574.html

(21 å°ç§ä¿¡ / 80 æ¡æ¶ˆæ¯) çŒ›çŒ¿ - çŸ¥ä¹
https://www.zhihu.com/people/lemonround

å¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒå¹¶è¡ŒæŠ€æœ¯ï¼ˆäºŒï¼‰-æ•°æ®å¹¶è¡Œ - æ˜é‡‘
https://juejin.cn/post/7254001262646738981

Megatron-LMå­¦ä¹ ï¼šPipline Parallelismçš„è®¾è®¡ä¸å®ç° - æ˜é‡‘
https://juejin.cn/post/7330916433559814184

å¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒå¹¶è¡ŒæŠ€æœ¯ï¼ˆä¸‰ï¼‰-æµæ°´çº¿å¹¶è¡Œ - æ˜é‡‘
https://juejin.cn/post/7262274383287484476

æ—‹è½¬ä½ç½®ç¼–ç RoPEçš„ç›´è§‚ç†è§£ - æ˜é‡‘
https://juejin.cn/post/7264920948758773812

å¤§æ¨¡å‹å¹¶è¡Œè®­ç»ƒæŒ‡å—ï¼šé€šä¿—ç†è§£Megatron-DeepSpeedä¹‹æ¨¡å‹å¹¶è¡Œä¸æ•°æ®å¹¶è¡Œ_è¯·è§£é‡Šä¸€ä¸‹æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œçš„åŒºåˆ«,ä»¥åŠå®ƒä»¬åœ¨å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸­çš„åº”ç”¨ã€‚-CSDNåšå®¢
https://blog.csdn.net/v_JULY_v/article/details/132462452

Megatron-LMæŠ€æœ¯è®²è§£ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/702532131

æ·±å…¥ç†è§£ Megatron-LMï¼ˆ4ï¼‰å¹¶è¡Œè®¾ç½® - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/650500590

æ·±å…¥ç†è§£ Megatron-LMï¼ˆ5ï¼‰å¼ é‡å¹¶è¡Œ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/650237833

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (5) --Pipedream Flush - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15890482.html

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (1) --- è®ºæ–‡ & åŸºç¡€ - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15840803.html

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (3) ---æ¨¡å‹å¹¶è¡Œå®ç° - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15871062.html

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (2) --- æ•´ä½“æ¶æ„ - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15868988.html

[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(2) - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/388830967

[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(1) - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/366906920

çŸ¥ä¹æ–¹ä½³ç‘_ç™¾åº¦æœç´¢
https://www.baidu.com/s?wd=%E7%9F%A5%E4%B9%8E%E6%96%B9%E4%BD%B3%E7%91%9E&tn=84053098_3_dg&ie=utf-8

æ··åˆåºåˆ—å¹¶è¡Œæ€è€ƒï¼šæœ‰å§é¾™çš„åœ°æ–¹å¿…æœ‰å‡¤é› - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/705835605

FlashAttention çš„é€Ÿåº¦ä¼˜åŒ–åŸç†æ˜¯æ€æ ·çš„ï¼Ÿ - çŸ¥ä¹
https://www.zhihu.com/question/611236756

https://arxiv.org/pdf/2406.06858
https://arxiv.org/pdf/2406.06858

mlsys blog

åˆ†å¸ƒå¼è®­ç»ƒ - å¤šæœºå¤šå¡ (DDP)-CSDNåšå®¢
https://blog.csdn.net/love1005lin/article/details/116456422

æµ…è°ˆåå‘ä¼ é€’çš„è®¡ç®—é‡å¤§çº¦æ˜¯å‰å‘ä¼ é€’çš„ä¸¤å€ - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/675517271?utm_source=wechat_session&utm_medium=social&s_r=0&wechatShare=1

[æºç åˆ†æ] Facebookå¦‚ä½•è®­ç»ƒè¶…å¤§æ¨¡å‹ --- (2) - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­
https://www.cnblogs.com/rossiXYZ/p/15819817.html

Facebookçš„ZeROç®—æ³•åŸç†åŠç®€å•ä»£ç å®éªŒï¼ˆå°æ˜¾å¡è®­å¤§æ¨¡å‹ï¼‰_zero3 æ¨¡å‹-CSDNåšå®¢
https://blog.csdn.net/weixin_43922901/article/details/126246309

è¯¦è§£PyTorch FSDPæ•°æ®å¹¶è¡Œ(Fully Sharded Data Parallel)-CSDNåšå®¢
https://blog.csdn.net/qinduohao333/article/details/131650137

Hugging Faceé«˜æ•ˆè®­ç»ƒæŠ€æœ¯äºŒï¼šå¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥â€”â€”ZeROã€FSDP_zero å¤§æ¨¡å‹-CSDNåšå®¢
https://blog.csdn.net/qq_56591814/article/details/133189752


